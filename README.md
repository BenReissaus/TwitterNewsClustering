# **Twitter News Clustering**

The goal of this project is to create a continuous feed of relevant world news, based on tweets retrieved from the Twitter API. Due to the potentially large amount of data, the news feed is generated by a distributed system, using Apache Spark and Scala. The project was developed in the context of the seminar "Mining Massive Datasets" by Masters students ([Daniel Neuschäfer-Rube](https://github.com/dneuschaefer-rube), [Jaqueline Pollak](https://github.com/JaquelineP), [Benjamin Reißaus](https://github.com/BenReissaus)) at Hasso Plattner Institute (Potsdam, Germany). The following sections outline how to run the project, details on the algorithms can be found on the wiki pages.

## **Project structure / features**

### **Generating the news feed**

1. Build the jar using `mvn package`

2. Start the clustering with one of two modes:

    1. Using a pre-generated dump of tweets ([generate yourself](https://github.com/JaquelineP/TwitterNewsClustering/blob/master/gather-tweets/README.md), or use [this sample](https://drive.google.com/file/d/0B1M9c5rlifEmUDRwcllZU3Y5SWc/view?usp=sharing)): `spark-submit --class de.hpi.isg.mmds.sparkstreaming.Main target\SparkTwitterClustering-jar-with-dependencies.jar -input [path to twitter.dat]`

    2. Using live tweets from the API (requires config.txt ([see here](https://github.com/JaquelineP/TwitterNewsClustering/blob/master/gather-tweets/README.md) for format) in the resources folder): `spark-submit --class de.hpi.isg.mmds.sparkstreaming.Main target\SparkTwitterClustering-jar-with-dependencies.jar -source api`

3. Results are printed to console, but can also be visualized (see next section)

## **How to run**

1. Build Jar
    * `mvn clean package`

2. Run Clustering:

    * Cluster tweets from file ([generate yourself](https://github.com/JaquelineP/TwitterNewsClustering/blob/master/gather-tweets/README.md), or use [this sample](https://drive.google.com/file/d/0B1M9c5rlifEmUDRwcllZU3Y5SWc/view?usp=sharing)): 
        * `spark-submit --class de.hpi.isg.mmds.sparkstreaming.Main target\SparkTwitterClustering-jar-with-dependencies.jar -input [path to twitter.dat]`

    * Cluster tweets from Twitter API (requires [config.txt](https://github.com/JaquelineP/TwitterNewsClustering/blob/master/gather-tweets/README.md) in resources folder): 
        * `spark-submit --class de.hpi.isg.mmds.sparkstreaming.Main target\SparkTwitterClustering-jar-with-dependencies.jar -source api`

3. Merge Clustering Results
	* `spark-submit --class de.hpi.isg.mmds.sparkstreaming.ClusterInfoAggregation target\SparkTwitterClustering-jar-with-dependencies.jar`
	
4. Visualize Clustering Results ([Details](https://github.com/JaquelineP/TwitterNewsClustering/tree/master/webapp))
    * install node
    * install node packages in `webapp` folder: `npm install`
	* run webserver: `node /webapp/server.js`
	* open browser: [http://localhost:3000/index](http://localhost:3000/index)

## **Cluster**

* Spark version: 1.6.1

* Scala version: 2.10.5

* Java version: 1.8

* [e.g. Amazon: cluster size, Cores]

### **Automation Scripts**

We have provided the following automation scripts in the folder `SparkTwitterClustering/scripts/`:

1. `buildAndCopyToCluster.sh` 
    * **actions**:
        *  build fat jar `SparkTwitterClustering-jar-with-dependencies.jar` on local machine 
        *  copy `SparkTwitterClustering-jar-with-dependencies.jar`, `runOnCluster.sh`, `driver_bootstrap.sh` and `twitter.dat` to remote server which is referenced by `MASTER_PUBLIC_DNS`.
    * **command**: `./buildAndCopyToCluster.sh [MASTER_PUBLIC_DNS]`
    * **execution machine**: local


2. `driver_bootstrap.sh`
    * **actions**:
        * install git, tmux, zsh, oh-my-zsh
        * configure tmux
        * copy twitter.dat to HDFS
    * **command**: `./driver_bootstrap.sh`
    * **execution machine**: remote cluster driver


3. `java8_bootstrap.sh`
    * **actions**:
        * install java 8 and set up JAVA_HOME accordingly
    * **command**: --- supplied during cluster creation
    * **execution machine**: all cluster nodes

4. `runOnCluster.sh`
    * **actions**:
        * run spark job with different batch sizes, number of executors, number of cores
        * save results in `runtime.csv`
    * **command**: `./runOnCluster.sh`
    * **execution machine**: remote cluster drive



## **Contributors**

[Daniel Neuschäfer-Rube](https://github.com/dneuschaefer-rube)

[Jaqueline Pollak](https://github.com/JaquelineP)

[Benjamin Reißaus](https://github.com/BenReissaus)
